#!/usr/bin/env python3
â€œâ€â€
Super RSS Feed Curator with Category-Based Feeds

- Categorizes articles into 8 topic-based feeds
- Generates individual JSON feeds per category
- Auto-generates OPML subscription file
- Uses Claude API with prompt caching for cost efficiency
  â€œâ€â€
  import os
  import sys
  import re
  from datetime import datetime, timedelta, timezone
  from collections import defaultdict
  from typing import List, Dict, Optional, Set
  import xml.etree.ElementTree as ET
  import hashlib
  import json
  from urllib.parse import urljoin, urlparse

import feedparser
from fuzzywuzzy import fuzz
import anthropic
import requests
from bs4 import BeautifulSoup

# Configuration

MAX_PER_CATEGORY = 50  # Max articles per category feed
MAX_PER_SOURCE = 8  # Default limit per source
MAX_PER_LOCAL = 15  # Higher limit for local content
LOOKBACK_HOURS = 48  # How far back to fetch articles
MIN_CLAUDE_SCORE = 30  # Minimum relevance score (0-100)
LOCAL_PRIORITY_SCORE = 100  # Maximum score for local articles

# Caching configuration

SCORED_CACHE_FILE = â€˜scored_articles_cache.jsonâ€™
WLT_CACHE_FILE = â€˜wlt_cache.jsonâ€™
CACHE_EXPIRY_HOURS = 24  # Donâ€™t re-score articles for 24 hours

# Williams Lake Tribune settings

WLT_BASE_URL = â€œhttps://wltribune.comâ€
WLT_NEWS_URL = fâ€{WLT_BASE_URL}/news/â€

# Category definitions

CATEGORIES = {
â€˜localâ€™: {
â€˜nameâ€™: â€˜Williams Lake Localâ€™,
â€˜emojiâ€™: â€˜ğŸ“â€™,
â€˜descriptionâ€™: â€˜Local news from Williams Lake and surrounding Cariboo regionâ€™,
â€˜filenameâ€™: â€˜feed-local.jsonâ€™
},
â€˜ai_techâ€™: {
â€˜nameâ€™: â€˜AI/ML & Tech Infrastructureâ€™,
â€˜emojiâ€™: â€˜ğŸ¤–â€™,
â€˜descriptionâ€™: â€˜AI, machine learning, infrastructure, and telemetryâ€™,
â€˜filenameâ€™: â€˜feed-ai-tech.jsonâ€™
},
â€˜climateâ€™: {
â€˜nameâ€™: â€˜Climate & Sustainabilityâ€™,
â€˜emojiâ€™: â€˜ğŸŒâ€™,
â€˜descriptionâ€™: â€˜Climate technology, sustainability, and environmental newsâ€™,
â€˜filenameâ€™: â€˜feed-climate.jsonâ€™
},
â€˜homelabâ€™: {
â€˜nameâ€™: â€˜Homelab & Self-Hostingâ€™,
â€˜emojiâ€™: â€˜ğŸ â€™,
â€˜descriptionâ€™: â€˜Homelab tech, self-hosting, and home automationâ€™,
â€˜filenameâ€™: â€˜feed-homelab.jsonâ€™
},
â€˜meshâ€™: {
â€˜nameâ€™: â€˜Mesh Networks & Hardwareâ€™,
â€˜emojiâ€™: â€˜ğŸ“¡â€™,
â€˜descriptionâ€™: â€˜Meshtastic, mesh networking, and hardware projectsâ€™,
â€˜filenameâ€™: â€˜feed-mesh.jsonâ€™
},
â€˜scienceâ€™: {
â€˜nameâ€™: â€˜Science & Systems Thinkingâ€™,
â€˜emojiâ€™: â€˜ğŸ”¬â€™,
â€˜descriptionâ€™: â€˜Systems thinking, complexity, and scientific discoveriesâ€™,
â€˜filenameâ€™: â€˜feed-science.jsonâ€™
},
â€˜scifiâ€™: {
â€˜nameâ€™: â€˜Sci-fi & Worldbuildingâ€™,
â€˜emojiâ€™: â€˜ğŸ“šâ€™,
â€˜descriptionâ€™: â€˜Science fiction, fantasy, and worldbuildingâ€™,
â€˜filenameâ€™: â€˜feed-scifi.jsonâ€™
},
â€˜newsâ€™: {
â€˜nameâ€™: â€˜Canadian and Global Newsâ€™,
â€˜emojiâ€™: â€˜ğŸŒâ€™,
â€˜descriptionâ€™: â€˜Canadian and international news coverageâ€™,
â€˜filenameâ€™: â€˜feed-news.jsonâ€™
}
}

# Filters

BLOCKED_SOURCES = [â€œfox newsâ€, â€œfoxnewsâ€]
BLOCKED_KEYWORDS = [
# Sports
â€œnflâ€, â€œnbaâ€, â€œmlbâ€, â€œnhlâ€, â€œpremier leagueâ€, â€œchampions leagueâ€,
â€œworld cupâ€, â€œolympicsâ€, â€œsuper bowlâ€, â€œplayoffâ€, â€œtouchdownâ€,
â€œhockeyâ€, â€œfootballâ€, â€œsoccerâ€, â€œbasketballâ€, â€œbaseballâ€,
â€œtournamentâ€, â€œchampionshipâ€, â€œsportsâ€, â€œathleticsâ€,
â€œrec centreâ€, â€œrecreation centreâ€, â€œarenaâ€,
# Advice columns
â€œdear abbyâ€, â€œask amyâ€, â€œmiss mannersâ€, â€œadvice columnâ€,
â€œrelationship adviceâ€, â€œdating adviceâ€
]

class Article:
â€œâ€â€œRepresents a single article with category tagsâ€â€â€
def **init**(self, entry=None, source_title: str = â€œâ€, source_url: str = â€œâ€,
title: str = â€œâ€, link: str = â€œâ€, description: str = â€œâ€,
pub_date: datetime = None, is_local: bool = False):

```
    # Handle both RSS entries and manual creation
    if entry:
        self.title = entry.get('title', '').strip()
        self.link = entry.get('link', '').strip()
        self.description = entry.get('description', '') or entry.get('summary', '')
        self.pub_date = self._parse_date(entry)
        self.image_url = self._extract_image(entry)
    else:
        self.title = title.strip()
        self.link = link.strip()
        self.description = description.strip()
        self.pub_date = pub_date or datetime.now(timezone.utc)
        self.image_url = None
    
    self.source = source_title
    self.source_url = source_url
    self.score = LOCAL_PRIORITY_SCORE if is_local else 0
    self.is_local = is_local
    self.categories: Set[str] = {'local'} if is_local else set()
    
    # Generate hash for deduplication and caching
    self.url_hash = hashlib.md5(self.link.encode()).hexdigest()
    self.title_normalized = self.title.lower().strip()

def _parse_date(self, entry) -> datetime:
    """Parse publication date from entry"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

def _extract_image(self, entry) -> Optional[str]:
    """Extract image URL from RSS feed entry"""
    # Check enclosures first (most common)
    if hasattr(entry, 'enclosures') and entry.enclosures:
        for enc in entry.enclosures:
            if enc.get('type', '').startswith('image/'):
                return enc.get('href') or enc.get('url')
    
    # Check media:content
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if media.get('medium') == 'image' or media.get('type', '').startswith('image/'):
                return media.get('url')
    
    # Check media:thumbnail
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        return entry.media_thumbnail[0].get('url')
    
    return None

def should_filter(self) -> bool:
    """Check if article should be filtered out (never filter local content)"""
    if self.is_local:
        return False
        
    text = f"{self.title} {self.description}".lower()
    
    # Check blocked sources
    source_lower = self.source.lower()
    if any(blocked in source_lower for blocked in BLOCKED_SOURCES):
        return True
    
    # Check blocked keywords
    if any(keyword in text for keyword in BLOCKED_KEYWORDS):
        return True
    
    return False
```

def load_wlt_cache() -> Dict[str, bool]:
â€œâ€â€œLoad Williams Lake Tribune URL cache to avoid re-scrapingâ€â€â€
try:
with open(WLT_CACHE_FILE, â€˜râ€™) as f:
cache = json.load(f)
print(fâ€ğŸ“– Loaded WLT cache with {len(cache)} URLsâ€)
return cache
except FileNotFoundError:
print(â€œğŸ“– No WLT cache found, starting freshâ€)
return {}
except Exception as e:
print(fâ€âš  WLT cache load error: {e}â€)
return {}

def save_wlt_cache(cache: Dict[str, bool]):
â€œâ€â€œSave Williams Lake Tribune URL cacheâ€â€â€
try:
with open(WLT_CACHE_FILE, â€˜wâ€™) as f:
json.dump(cache, f, indent=2)
print(fâ€ğŸ’¾ Saved WLT cache with {len(cache)} URLsâ€)
except Exception as e:
print(fâ€âš  WLT cache save error: {e}â€)

def scrape_williams_lake_tribune() -> List[Article]:
â€œâ€â€œScrape Williams Lake Tribune directly for priority local newsâ€â€â€
print(â€œğŸ“ Scraping Williams Lake Tribuneâ€¦â€)

```
cache = load_wlt_cache()
articles = []
new_urls = []

# Sports keywords for filtering even local content
sports_keywords = ["hockey", "basketball", "soccer", "football", "baseball", 
                  "tournament", "championship", "sports", "athletics",
                  "game", "season", "playoff", "league"]

try:
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(WLT_NEWS_URL, headers=headers, timeout=10)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.content, 'html.parser')
    article_links = soup.find_all('a', href=True)
    
    for link in article_links:
        href = link.get('href', '')
        
        if not href or href.startswith('#') or 'javascript:' in href:
            continue
            
        if href.startswith('/'):
            full_url = urljoin(WLT_BASE_URL, href)
        elif href.startswith('http'):
            full_url = href
        else:
            continue
        
        if 'wltribune.com' not in full_url or '/2026/' not in full_url:
            continue
            
        if full_url in cache:
            continue
        
        title = link.get_text().strip()
        if not title or len(title) < 10:
            continue
            
        # Check if title contains sports keywords (even for local content)
        title_lower = title.lower()
        if any(keyword in title_lower for keyword in sports_keywords):
            continue  # Skip sports articles even from WLT
            
        article = Article(
            title=title,
            link=full_url,
            description=f"Local news from Williams Lake Tribune",
            pub_date=datetime.now(timezone.utc),
            source_title="Williams Lake Tribune",
            source_url=WLT_BASE_URL,
            is_local=True
        )
        
        articles.append(article)
        new_urls.append(full_url)
        cache[full_url] = True
        
        if len(articles) >= 20:
            break
    
    if new_urls:
        save_wlt_cache(cache)
        
    print(f"  ğŸ“° Williams Lake Tribune: {len(articles)} new articles")

except Exception as e:
    print(f"  âœ— Williams Lake Tribune scraping error: {e}")

return articles
```

def load_scored_cache() -> Dict[str, Dict]:
â€œâ€â€œLoad cache of previously scored articlesâ€â€â€
try:
with open(SCORED_CACHE_FILE, â€˜râ€™) as f:
cache = json.load(f)
print(fâ€ğŸ“– Loaded {len(cache)} articles from scoring cacheâ€)
return cache
except FileNotFoundError:
print(â€œğŸ“– No scoring cache found, starting freshâ€)
return {}
except Exception as e:
print(fâ€âš  Scoring cache load error: {e}â€)
return {}

def save_scored_cache(cache: Dict[str, Dict]):
â€œâ€â€œSave cache of scored articles, removing old entriesâ€â€â€
try:
cutoff_time = datetime.now(timezone.utc) - timedelta(hours=12)
cleaned_cache = {
url_hash: data for url_hash, data in cache.items()
if datetime.fromisoformat(data[â€˜scored_atâ€™]) > cutoff_time
}

```
    with open(SCORED_CACHE_FILE, 'w') as f:
        json.dump(cleaned_cache, f, indent=2)
    
    removed = len(cache) - len(cleaned_cache)
    print(f"ğŸ’¾ Saved scoring cache with {len(cleaned_cache)} articles" + 
          (f" (removed {removed} old entries)" if removed > 0 else ""))
except Exception as e:
    print(f"âš  Scoring cache save error: {e}")
```

def is_cache_entry_valid(cache_entry: Dict) -> bool:
â€œâ€â€œCheck if cached score is still valid (not expired)â€â€â€
try:
scored_time = datetime.fromisoformat(cache_entry[â€˜scored_atâ€™])
expiry_time = scored_time + timedelta(hours=CACHE_EXPIRY_HOURS)
return datetime.now(timezone.utc) < expiry_time
except:
return False

def parse_opml(opml_path: str) -> List[Dict[str, str]]:
â€œâ€â€œExtract RSS feed URLs from OPML fileâ€â€â€
feeds = []
tree = ET.parse(opml_path)
root = tree.getroot()

```
for outline in root.findall(".//outline[@type='rss']"):
    feed_url = outline.get('xmlUrl')
    feed_title = outline.get('title') or outline.get('text')
    html_url = outline.get('htmlUrl', '')
    
    if feed_url:
        feeds.append({
            'url': feed_url,
            'title': feed_title,
            'html_url': html_url
        })

print(f"ğŸ“š Found {len(feeds)} feeds in OPML")
return feeds
```

def fetch_feed_articles(feed: Dict[str, str], cutoff_date: datetime) -> List[Article]:
â€œâ€â€œFetch recent articles from a single feedâ€â€â€
articles = []

```
try:
    parsed = feedparser.parse(feed['url'])
    
    for entry in parsed.entries:
        article = Article(entry, feed['title'], feed['html_url'])
        
        if article.pub_date < cutoff_date:
            continue
        
        if article.should_filter():
            continue
        
        articles.append(article)
    
    if articles:
        print(f"  âœ“ {feed['title']}: {len(articles)} articles")

except Exception as e:
    print(f"  âœ— {feed['title']}: {str(e)}")

return articles
```

def deduplicate_articles(articles: List[Article]) -> List[Article]:
â€œâ€â€œRemove duplicate articles using URL and fuzzy title matchingâ€â€â€
seen_urls = set()
seen_titles = []
unique = []

```
for article in articles:
    if article.url_hash in seen_urls:
        continue
    
    is_duplicate = False
    for seen_title in seen_titles:
        if fuzz.ratio(article.title_normalized, seen_title) > 75:
            is_duplicate = True
            break
    
    if is_duplicate:
        continue
    
    seen_urls.add(article.url_hash)
    seen_titles.append(article.title_normalized)
    unique.append(article)

print(f"ğŸ” Deduplication: {len(articles)} â†’ {len(unique)} articles")
return unique
```

def score_and_categorize_articles(articles: List[Article], api_key: str) -> List[Article]:
â€œâ€â€œScore articles and assign categories using Claude API with prompt cachingâ€â€â€

```
# Separate local articles (already categorized) from others
local_articles = [a for a in articles if a.is_local]
non_local_articles = [a for a in articles if not a.is_local]

if local_articles:
    print(f"ğŸ“ Local articles: {len(local_articles)} (auto-scored and categorized)")

if not non_local_articles:
    print("ğŸ” Only local articles found, skipping Claude scoring")
    return articles

# Load cache
cache = load_scored_cache()

# Separate cached vs new articles
cached_articles = []
new_articles = []
cache_hits = 0

for article in non_local_articles:
    cache_entry = cache.get(article.url_hash)
    if cache_entry and is_cache_entry_valid(cache_entry):
        # Use cached score and categories
        article.score = cache_entry['score']
        article.categories = set(cache_entry.get('categories', []))
        cached_articles.append(article)
        cache_hits += 1
    else:
        new_articles.append(article)

print(f"ğŸ’¡ Cache: {cache_hits} hits, {len(new_articles)} new articles to score")

# Only score new articles if there are any
if new_articles:
    client = anthropic.Anthropic(api_key=api_key)
    
    # System prompt with cache control (cached across all API calls!)
    system_prompt = [{
        "type": "text",
        "text": """You are an article scorer and categorizer. For each article, provide:
```

1. Relevance score (0-100) based on these interests:
- AI/ML infrastructure and telemetry
- Systems thinking and complex systems
- Climate tech and sustainability
- Homelab/self-hosting technology
- Meshtastic and mesh networking
- 3D printing (Bambu Lab)
- Sci-fi worldbuilding
- Deep technical content over news
- Canadian content and local news
1. Category tags (can assign multiple):
- ai_tech: AI/ML, infrastructure, telemetry, tech platforms
- climate: Climate, sustainability, environment, clean energy
- homelab: Home automation, self-hosting, HomeLab, DIY tech
- mesh: Meshtastic, LoRa, mesh networks, radio, networking hardware
- science: Systems thinking, complexity, scientific research
- scifi: Science fiction, fantasy, worldbuilding, speculative fiction
- news: Canadian news, global news, politics, current events

Return ONLY valid JSON (no markdown, no backticks):
[{â€œscoreâ€: 85, â€œcategoriesâ€: [â€œai_techâ€, â€œscienceâ€]}, {â€œscoreâ€: 42, â€œcategoriesâ€: [â€œnewsâ€]}, â€¦]â€â€â€,
â€œcache_controlâ€: {â€œtypeâ€: â€œephemeralâ€}
}]

```
    print(f"ğŸ¤– Scoring and categorizing {len(new_articles)} articles with Claude (prompt caching enabled)...")
    
    # Batch articles for efficiency (10 at a time)
    batch_size = 10
    
    for i in range(0, len(new_articles), batch_size):
        batch = new_articles[i:i+batch_size]
        
        # Prepare batch for Claude
        article_list = "\n\n".join([
            f"Article {idx}:\nTitle: {a.title}\nSource: {a.source}\nDescription: {a.description[:200]}..."
            for idx, a in enumerate(batch)
        ])
        
        try:
            response = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1000,
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": f"Score and categorize these articles:\n\n{article_list}"
                }]
            )
            
            result_text = response.content[0].text.strip()
            
            # Parse JSON response
            results = json.loads(result_text)
            
            # Apply scores and categories
            current_time = datetime.now(timezone.utc).isoformat()
            for article, result in zip(batch, results):
                article.score = result.get('score', 50)
                article.categories = set(result.get('categories', ['news']))
                
                # Update cache
                cache[article.url_hash] = {
                    'score': article.score,
                    'categories': list(article.categories),
                    'title': article.title,
                    'source': article.source,
                    'scored_at': current_time
                }
        
        except Exception as e:
            print(f"  âš  Scoring error: {e}")
            # Assign default scores on error
            for article in batch:
                article.score = 50
                article.categories = {'news'}

# Save updated cache
if new_articles:
    save_scored_cache(cache)

# Return all articles (local + cached + newly scored)
all_articles = local_articles + cached_articles + new_articles
return all_articles
```

def apply_diversity_limits(articles: List[Article], max_per_source: int) -> List[Article]:
â€œâ€â€œLimit articles per source to ensure diversityâ€â€â€
source_counts = defaultdict(int)
diverse_articles = []

```
# Sort by score first
sorted_articles = sorted(articles, key=lambda a: a.score, reverse=True)

for article in sorted_articles:
    limit = MAX_PER_LOCAL if article.is_local else max_per_source
    
    if source_counts[article.source] < limit:
        diverse_articles.append(article)
        source_counts[article.source] += 1

print(f"ğŸ“Š Diversity filter: {len(articles)} â†’ {len(diverse_articles)} articles")
return diverse_articles
```

def generate_category_feeds(articles: List[Article], base_url: str):
â€œâ€â€œGenerate individual JSON feeds per categoryâ€â€â€

```
# Organize articles by category
category_articles = defaultdict(list)
for article in articles:
    for category in article.categories:
        if category in CATEGORIES:
            category_articles[category].append(article)

print(f"\nğŸ“‚ Generating category feeds...")

# Generate feed for each category
for cat_key, cat_info in CATEGORIES.items():
    cat_articles = category_articles.get(cat_key, [])
    
    if not cat_articles:
        print(f"  âš  {cat_info['emoji']} {cat_info['name']}: 0 articles (skipping)")
        continue
    
    # Sort by score and limit
    cat_articles.sort(key=lambda a: a.score, reverse=True)
    cat_articles = cat_articles[:MAX_PER_CATEGORY]
    
    feed_data = {
        "version": "https://jsonfeed.org/version/1.1",
        "title": f"{cat_info['emoji']} {cat_info['name']}",
        "home_page_url": "https://github.com/zirnhelt/super-rss-feed",
        "feed_url": f"{base_url}/{cat_info['filename']}",
        "description": cat_info['description'],
        "authors": [{"name": "Erich's AI Curator"}],
        "items": []
    }
    
    for article in cat_articles:
        local_prefix = "ğŸ“ " if article.is_local else ""
        item_title = f"{local_prefix}[{article.source}] {article.title}"
        
        item = {
            "id": article.link,
            "url": article.link,
            "title": item_title,
            "content_html": f"<p>{article.description}</p>",
            "summary": article.description,
            "date_published": article.pub_date.isoformat(),
            "authors": [{"name": article.source}],
            "tags": [article.source.lower().replace(" ", "_")] + list(article.categories),
            "_source": {
                "original_title": article.title,
                "source_name": article.source,
                "source_url": article.source_url,
                "ai_score": article.score,
                "categories": list(article.categories),
                "is_local": article.is_local
            }
        }
        
        if article.source_url:
            item["external_url"] = article.source_url
        
        if article.image_url:
            item["image"] = article.image_url
            
        feed_data["items"].append(item)
    
    # Write JSON file
    with open(cat_info['filename'], 'w', encoding='utf-8') as f:
        json.dump(feed_data, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ“ {cat_info['emoji']} {cat_info['name']}: {len(cat_articles)} articles â†’ {cat_info['filename']}")
```

def generate_opml(base_url: str):
â€œâ€â€œGenerate OPML subscription file for all category feedsâ€â€â€

```
opml = ET.Element('opml', version="1.0")
head = ET.SubElement(opml, 'head')
ET.SubElement(head, 'title').text = "Erich's Curated Feeds"
ET.SubElement(head, 'dateCreated').text = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S GMT')

body = ET.SubElement(opml, 'body')
 
# Add category feeds
for cat_key, cat_info in CATEGORIES.items():
    ET.SubElement(body, 'outline',
                  type="rss",
                  text=f"{cat_info['emoji']} {cat_info['name']}",
                  title=f"{cat_info['emoji']} {cat_info['name']}",
                  xmlUrl=f"{base_url}/{cat_info['filename']}",
                  htmlUrl="https://github.com/zirnhelt/super-rss-feed")

# Write OPML file
tree = ET.ElementTree(opml)
ET.indent(tree, space="  ")
tree.write('curated-feeds.opml', encoding='utf-8', xml_declaration=True)

print(f"\nğŸ“‹ OPML subscription file â†’ curated-feeds.opml")
print(f"   Import this into Inoreader to subscribe to all feeds at once!")
```

def main():
# Check for API key
api_key = os.getenv(â€˜ANTHROPIC_API_KEYâ€™)
if not api_key:
print(â€œâŒ Error: ANTHROPIC_API_KEY environment variable not setâ€)
sys.exit(1)

```
# Parse OPML
opml_path = sys.argv[1] if len(sys.argv) > 1 else 'feeds.opml'
feeds = parse_opml(opml_path)

# Step 1: Scrape Williams Lake Tribune
local_articles = scrape_williams_lake_tribune()

# Step 2: Fetch articles from OPML feeds
cutoff_date = datetime.now(timezone.utc) - timedelta(hours=LOOKBACK_HOURS)
print(f"\nğŸ”¥ Fetching articles from last {LOOKBACK_HOURS} hours...")

rss_articles = []
for feed in feeds:
    if 'wltribune' in feed['url'].lower():
        print(f"  â­ {feed['title']}: Skipped (using direct scraper)")
        continue
        
    articles = fetch_feed_articles(feed, cutoff_date)
    rss_articles.extend(articles)

# Combine and process
all_articles = local_articles + rss_articles
print(f"\nğŸ“ˆ Total fetched: {len(all_articles)} articles ({len(local_articles)} local + {len(rss_articles)} RSS)")

# Deduplicate
unique_articles = deduplicate_articles(all_articles)

# Score and categorize with Claude
scored_articles = score_and_categorize_articles(unique_articles, api_key)

# Filter by minimum score (but always include local)
quality_articles = [a for a in scored_articles if a.score >= MIN_CLAUDE_SCORE or a.is_local]
print(f"â­ Quality filter (score >= {MIN_CLAUDE_SCORE}): {len(scored_articles)} â†’ {len(quality_articles)} articles")

# Apply diversity limits
diverse_articles = apply_diversity_limits(quality_articles, MAX_PER_SOURCE)

# Generate category feeds
base_url = "https://zirnhelt.github.io/super-rss-feed"
generate_category_feeds(diverse_articles, base_url)

# Generate OPML subscription file
generate_opml(base_url)

# Stats
print("\nğŸ“Š Final stats:")
print(f"  Total sources: {len(feeds) + 1}")
print(f"  Articles fetched: {len(all_articles)}")
print(f"  After dedup: {len(unique_articles)}")
print(f"  After scoring: {len(quality_articles)}")
print(f"  After diversity: {len(diverse_articles)}")

# Category breakdown
print("\nğŸ“‚ Articles by category:")
category_counts = defaultdict(int)
for article in diverse_articles:
    for cat in article.categories:
        if cat in CATEGORIES:
            category_counts[cat] += 1

for cat_key in CATEGORIES.keys():
    count = category_counts.get(cat_key, 0)
    emoji = CATEGORIES[cat_key]['emoji']
    name = CATEGORIES[cat_key]['name']
    print(f"  {emoji} {name}: {count} articles")
```

if **name** == â€˜**main**â€™:
main()
